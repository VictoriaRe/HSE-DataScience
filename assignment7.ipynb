{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment7.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VictoriaRe/HSE-DataScience/blob/master/assignment7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLI659rQlHtB",
        "colab_type": "code",
        "outputId": "b1d57e7f-38d3-40f4-8a21-fc7620c4c776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "!pip3 install tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcmk90AOwbh9",
        "colab_type": "code",
        "outputId": "c4480dbe-93e6-4358-8b46-acafe8e8b06b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfJ_f5-9FfaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI0N4TgI5p95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchtext import datasets, data\n",
        "import sentencepiece as spm\n",
        "from torch.autograd import Variable\n",
        "\n",
        "#DEVICE = 'cuda'\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdsPf3W77NHN",
        "colab_type": "code",
        "outputId": "52c08d12-92c8-4c2e-b4f9-5a0a026e5940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# tokenize english \n",
        "with open('news-commentary-v13.ru-en.en') as f:\n",
        "    with open('text.en', 'w') as out:\n",
        "            out.write(f.read().lower())\n",
        "        \n",
        "spm.SentencePieceTrainer.Train('--input=text.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgMybRU1BCAj",
        "colab_type": "code",
        "outputId": "3cf11eea-1127-4353-c3f1-8dbe98403ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# tokenize russian \n",
        "with open('news-commentary-v13.ru-en.ru') as f:\n",
        "    with open('text.ru', 'w') as out:\n",
        "            out.write(f.read().lower())\n",
        "        \n",
        "spm.SentencePieceTrainer.Train('--input=text.ru --model_prefix=bpe_ru --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jOWsPxR7b7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_ru = spm.SentencePieceProcessor()\n",
        "tok_ru.load('bpe_ru.model')\n",
        "\n",
        "tok_en = spm.SentencePieceProcessor()\n",
        "tok_en.load('bpe_en.model')\n",
        "\n",
        "SRC = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_ru.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "TGT = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "fields = (('src', SRC), ('tgt', TGT))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiiqn7CSAZf-",
        "colab_type": "code",
        "outputId": "3326287a-65ce-48bd-e28d-5254419c4a67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('text.ru') as f:\n",
        "    src_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "with open('text.en') as f:\n",
        "    tgt_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "examples = [data.Example.fromlist(x, fields) for x in tqdm(zip(src_snt, tgt_snt))]\n",
        "test = data.Dataset(examples[-1000:], fields)\n",
        "train, valid = data.Dataset(examples[:-1000], fields).split(0.9)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "235159it [01:10, 3313.35it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duK-OOYJDpQV",
        "colab_type": "code",
        "outputId": "1a9c5930-c94e-46d0-f990-95b29fc50ffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "print('src: ' + \" \".join(train.examples[100].src))\n",
        "print('tgt: ' + \" \".join(train.examples[100].tgt))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src: ▁после ▁вступления ▁на ▁главную ▁руководя щую ▁должность ▁в ▁китае ▁хи ▁цзиньпина , ▁которого ▁народно - освобод ительная ▁армия ▁китая ▁ ( но ак ) ▁считает ▁своим ▁человеком , ▁в ▁японии ▁на ▁предстоящих ▁выборах , ▁вероятнее ▁всего , ▁произойдет ▁смещение ▁вправо ▁ – ▁результатом ▁чего , ▁скорее ▁всего , ▁станет ▁усиление ▁националисти ческих ▁стра стей ▁по ▁обе ▁стороны ▁китайско - япо нского ▁соперничества .\n",
            "tgt: ▁after ▁the ▁ascension ▁in ▁china ▁of ▁ x i ▁ j inping , ▁regarded ▁by ▁the ▁people ’ s ▁liberation ▁army ▁ ( pla ) ▁as ▁its ▁own ▁man , ▁ j apan ▁seems ▁set ▁to ▁swing ▁to ▁the ▁right ▁in ▁its ▁impending ▁election ▁ – ▁an ▁outcome ▁likely ▁to ▁fuel ▁nationalist ▁passion ▁on ▁both ▁sides ▁of ▁the ▁sino -j apanese ▁rivalry .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqC8bX3hGsRo",
        "colab_type": "code",
        "outputId": "ee64c430-6bbe-45a1-d4a8-0833a3b6c146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train), len(valid), len(test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(210743, 23416, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WYFd2tIGvKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TGT.build_vocab(train, min_freq=5)\n",
        "SRC.build_vocab(train, min_freq=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NptTM0PCo57V",
        "colab_type": "text"
      },
      "source": [
        "The code below is the one from the article (Vaswani,  Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, Polosukhin, 2017), retrieved from http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "\n",
        "The author of this current code is expressing her sincere gratitude! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF6dFP-po02-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask,\n",
        "                            tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etsNhMB1p3Pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW0cSzvDp7Bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5FMRgS3p9w9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYXYCObSqA2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iim8D4vnthZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7NfWcPptprD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7igm5HktsdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBhXwgQ6twO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4tcMy6bty_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / np.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        try:\n",
        "          scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        except RuntimeError:\n",
        "          pass\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtb1jk8nt_BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVboT9OyuDxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLKU_JnkuKfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * np.sqrt(self.d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLf3q1yzuSnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce-y7mX2dQRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqtFljuV2to8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(\n",
        "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        return tgt_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzg24pr2uWkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, N=6, \n",
        "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                             c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        nn.Linear(d_model, tgt_vocab))\n",
        "        #Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJdeEb1yuyMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BucketIteratorWrapper(DataLoader):\n",
        "    __initialized = False\n",
        "\n",
        "    def __init__(self, iterator: data.Iterator):\n",
        "#         super(BucketIteratorWrapper,self).__init__()\n",
        "        self.batch_size = iterator.batch_size\n",
        "        self.num_workers = 1\n",
        "        self.collate_fn = None\n",
        "        self.pin_memory = False\n",
        "        self.drop_last = False\n",
        "        self.timeout = 0\n",
        "        self.worker_init_fn = None\n",
        "        self.sampler = iterator\n",
        "        self.batch_sampler = iterator\n",
        "        self.__initialized = True\n",
        "\n",
        "    def __iter__(self):\n",
        "        return map(\n",
        "            lambda batch: Batch(batch.src, batch.tgt, pad=TGT.vocab.stoi['<pad>']),\n",
        "            self.batch_sampler.__iter__()\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch_sampler)\n",
        "    \n",
        "class MyCriterion:\n",
        "    def __init__(self, generator, pad_idx):\n",
        "        self.generator = generator\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_idx)\n",
        "        self.criterion.cuda()\n",
        "        \n",
        "    def __call__(self, x, batch):\n",
        "        y = batch.trg_y\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.reshape(-1, x.size(-1)), \n",
        "                              y.reshape(-1))  / batch.ntokens\n",
        "        return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA3wYnouiKZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVw3sDh7cwwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 125\n",
        "num_epochs = 5\n",
        "pad_idx = TGT.vocab.stoi['<pad>']\n",
        "\n",
        "train_iter, valid_iter, test_iter = data.BucketIterator.splits((train, valid, test), \n",
        "                                              batch_sizes=(batch_size, batch_size, batch_size), \n",
        "                                  sort_key=lambda x: len(x.src),\n",
        "                                  shuffle=True,\n",
        "                                  device=DEVICE,\n",
        "                                  sort_within_batch=False)\n",
        "                                  \n",
        "train_iter = BucketIteratorWrapper(train_iter)\n",
        "valid_iter = BucketIteratorWrapper(valid_iter)\n",
        "test_iter = BucketIteratorWrapper(test_iter)\n",
        "\n",
        "model = make_model(len(SRC.vocab), len(TGT.vocab), N=2)\n",
        "model = model.to(DEVICE)\n",
        "criterion = MyCriterion(model.generator, pad_idx)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jlw6mSz0l1_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "02bf4d6d-a93f-4ae1-c577-c5c76230158f"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(valid_iter):\n",
        "        src = batch.src\n",
        "        src_key_padding_mask = src != SRC.vocab.stoi[\"<pad>\"]\n",
        "        beam = beam_search(model, src, src_key_padding_mask)\n",
        "        seq = []\n",
        "        for i in range(1, src.size(1)):\n",
        "            sym = SRC.vocab.itos[src[0, i]]\n",
        "            if sym == \"</s>\": break\n",
        "            seq.append(sym)\n",
        "        seq = tok_ru.decode_pieces(seq)\n",
        "        print(\"\\nSource:\", seq)\n",
        "        \n",
        "        print(\"Translation:\")\n",
        "        for pred, pred_proba in beam:                \n",
        "            seq = []\n",
        "            for i in range(1, pred.size(1)):\n",
        "                sym = TGT.vocab.itos[pred[0, i]]\n",
        "                if sym == \"</s>\": break\n",
        "                seq.append(sym)\n",
        "            seq = tok_en.decode_pieces(seq)\n",
        "            print(f\"pred {pred_proba:.2f}:\", seq)\n",
        "                \n",
        "        seq = []\n",
        "        for i in range(1, batch.trg.size(1)):\n",
        "            sym = TGT.vocab.itos[batch.trg[0, i]]\n",
        "            if sym == \"</s>\": break\n",
        "            seq.append(sym)\n",
        "        seq = tok_en.decode_pieces(seq)\n",
        "        print(\"Target:\", seq)\n",
        "        break"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-dac147e80a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msrc_key_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mSRC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<pad>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mbeam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-a8c4f7f7cb37>\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(model, src, src_mask, max_len, k)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstart_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTGT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<s>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTGT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mbeam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: fill_() received an invalid combination of arguments - got (tuple), but expected one of:\n * (Tensor value)\n      didn't match because some of the arguments have invalid types: (!tuple!)\n * (Number value)\n      didn't match because some of the arguments have invalid types: (!tuple!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxpD1LUU2k56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(data_iter, model, criterion):\n",
        "    total_loss = 0\n",
        "    data_iter = tqdm(data_iter)\n",
        "    counter = 0\n",
        "    for batch in data_iter:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(batch)\n",
        "        loss = criterion(pred, batch.tgt_y)\n",
        "        loss.backward()\n",
        "        curr_loss = loss.data.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += curr_loss\n",
        "        data_iter.set_postfix(loss=curr_loss)\n",
        "        counter +=1\n",
        "\n",
        "    total_loss /= counter\n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xEjRrCl2miM",
        "colab_type": "code",
        "outputId": "7ce0d803-e768-45f3-8123-79d2199ac2a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def valid_epoch(data_iter, model, criterion):\n",
        "    total_loss = 0\n",
        "    data_iter = tqdm(data_iter)\n",
        "    counter = 0\n",
        "    for batch in data_iter:\n",
        "        pred = model(batch)\n",
        "        curr_loss = criterion(pred, batch.tgt_y).data.item()\n",
        "\n",
        "        total_loss += curr_loss\n",
        "        data_iter.set_postfix(loss=curr_loss)\n",
        "        counter += 1\n",
        "\n",
        "    total_loss /= counter\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    loss = train_epoch(train_iter, model, criterion)\n",
        "    print(f'\\ntrain: {loss}\\n')\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss = valid_epoch(valid_iter, model, criterion)\n",
        "        scheduler.step(loss)\n",
        "        print(f'\\nvalid: {loss}\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1686 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 0/1686 [00:38<?, ?it/s, loss=10.3]\u001b[A\n",
            "  0%|          | 1/1686 [00:38<17:55:51, 38.31s/it, loss=10.3]\u001b[A\n",
            "  0%|          | 1/1686 [01:12<17:55:51, 38.31s/it, loss=9.53]\u001b[A\n",
            "  0%|          | 2/1686 [01:12<17:19:16, 37.03s/it, loss=9.53]\u001b[A\n",
            "  0%|          | 2/1686 [01:45<17:19:16, 37.03s/it, loss=8.99]\u001b[A\n",
            "  0%|          | 3/1686 [01:45<16:44:49, 35.82s/it, loss=8.99]\u001b[A\n",
            "  0%|          | 3/1686 [02:20<16:44:49, 35.82s/it, loss=9.23]\u001b[A\n",
            "  0%|          | 4/1686 [02:20<16:38:57, 35.63s/it, loss=9.23]\u001b[A\n",
            "  0%|          | 4/1686 [02:58<16:38:57, 35.63s/it, loss=9.88]\u001b[A\n",
            "  0%|          | 5/1686 [02:58<16:58:50, 36.37s/it, loss=9.88]\u001b[A\n",
            "  0%|          | 5/1686 [03:39<16:58:50, 36.37s/it, loss=9.54]\u001b[A\n",
            "  0%|          | 6/1686 [03:39<17:34:57, 37.68s/it, loss=9.54]\u001b[A\n",
            "  0%|          | 6/1686 [04:22<17:34:57, 37.68s/it, loss=8.86]\u001b[A\n",
            "  0%|          | 7/1686 [04:22<18:19:43, 39.30s/it, loss=8.86]\u001b[A\n",
            "  0%|          | 7/1686 [05:06<18:19:43, 39.30s/it, loss=8.62]\u001b[A\n",
            "  0%|          | 8/1686 [05:06<19:00:54, 40.80s/it, loss=8.62]\u001b[A\n",
            "  0%|          | 8/1686 [05:48<19:00:54, 40.80s/it, loss=8.96]\u001b[A\n",
            "  1%|          | 9/1686 [05:48<19:09:12, 41.12s/it, loss=8.96]\u001b[A\n",
            "  1%|          | 9/1686 [06:30<19:09:12, 41.12s/it, loss=8.47]\u001b[A\n",
            "  1%|          | 10/1686 [06:30<19:13:26, 41.29s/it, loss=8.47]\u001b[A\n",
            "  1%|          | 10/1686 [07:12<19:13:26, 41.29s/it, loss=8.19]\u001b[A\n",
            "  1%|          | 11/1686 [07:12<19:21:46, 41.62s/it, loss=8.19]\u001b[A\n",
            "  1%|          | 11/1686 [07:53<19:21:46, 41.62s/it, loss=8.03]\u001b[A\n",
            "  1%|          | 12/1686 [07:53<19:18:11, 41.51s/it, loss=8.03]\u001b[A\n",
            "  1%|          | 12/1686 [08:34<19:18:11, 41.51s/it, loss=7.9] \u001b[A\n",
            "  1%|          | 13/1686 [08:34<19:13:18, 41.36s/it, loss=7.9]\u001b[A\n",
            "  1%|          | 13/1686 [09:15<19:13:18, 41.36s/it, loss=7.82]\u001b[A\n",
            "  1%|          | 14/1686 [09:15<19:09:53, 41.26s/it, loss=7.82]\u001b[A\n",
            "  1%|          | 14/1686 [09:55<19:09:53, 41.26s/it, loss=7.79]\u001b[A\n",
            "  1%|          | 15/1686 [09:55<18:55:40, 40.78s/it, loss=7.79]\u001b[A\n",
            "  1%|          | 15/1686 [10:35<18:55:40, 40.78s/it, loss=7.82]\u001b[A\n",
            "  1%|          | 16/1686 [10:35<18:50:31, 40.62s/it, loss=7.82]\u001b[A\n",
            "  1%|          | 16/1686 [11:16<18:50:31, 40.62s/it, loss=7.77]\u001b[A\n",
            "  1%|          | 17/1686 [11:16<18:46:56, 40.51s/it, loss=7.77]\u001b[A\n",
            "  1%|          | 17/1686 [11:55<18:46:56, 40.51s/it, loss=7.6] \u001b[A\n",
            "  1%|          | 18/1686 [11:55<18:35:00, 40.11s/it, loss=7.6]\u001b[A\n",
            "  1%|          | 18/1686 [12:34<18:35:00, 40.11s/it, loss=7.71]\u001b[A\n",
            "  1%|          | 19/1686 [12:34<18:29:28, 39.93s/it, loss=7.71]\u001b[A\n",
            "  1%|          | 19/1686 [13:15<18:29:28, 39.93s/it, loss=7.62]\u001b[A\n",
            "  1%|          | 20/1686 [13:16<18:39:03, 40.30s/it, loss=7.62]\u001b[A\n",
            "  1%|          | 20/1686 [13:56<18:39:03, 40.30s/it, loss=7.72]\u001b[A\n",
            "  1%|          | 21/1686 [13:56<18:42:42, 40.46s/it, loss=7.72]\u001b[A\n",
            "  1%|          | 21/1686 [14:37<18:42:42, 40.46s/it, loss=7.58]\u001b[A\n",
            "  1%|▏         | 22/1686 [14:37<18:42:38, 40.48s/it, loss=7.58]\u001b[A\n",
            "  1%|▏         | 22/1686 [15:19<18:42:38, 40.48s/it, loss=7.54]\u001b[A\n",
            "  1%|▏         | 23/1686 [15:19<18:56:36, 41.01s/it, loss=7.54]\u001b[A\n",
            "  1%|▏         | 23/1686 [16:00<18:56:36, 41.01s/it, loss=7.49]\u001b[A\n",
            "  1%|▏         | 24/1686 [16:00<18:58:15, 41.09s/it, loss=7.49]\u001b[A\n",
            "  1%|▏         | 24/1686 [16:43<18:58:15, 41.09s/it, loss=7.46]\u001b[A\n",
            "  1%|▏         | 25/1686 [16:43<19:10:51, 41.57s/it, loss=7.46]\u001b[A\n",
            "  1%|▏         | 25/1686 [17:27<19:10:51, 41.57s/it, loss=7.49]\u001b[A\n",
            "  2%|▏         | 26/1686 [17:27<19:30:17, 42.30s/it, loss=7.49]\u001b[A\n",
            "  2%|▏         | 26/1686 [18:13<19:30:17, 42.30s/it, loss=7.57]\u001b[A\n",
            "  2%|▏         | 27/1686 [18:13<19:57:04, 43.29s/it, loss=7.57]\u001b[A\n",
            "  2%|▏         | 27/1686 [18:57<19:57:04, 43.29s/it, loss=7.48]\u001b[A\n",
            "  2%|▏         | 28/1686 [18:57<20:03:56, 43.57s/it, loss=7.48]\u001b[A\n",
            "  2%|▏         | 28/1686 [19:44<20:03:56, 43.57s/it, loss=7.45]\u001b[A\n",
            "  2%|▏         | 29/1686 [19:44<20:35:27, 44.74s/it, loss=7.45]\u001b[A\n",
            "  2%|▏         | 29/1686 [20:32<20:35:27, 44.74s/it, loss=7.35]\u001b[A\n",
            "  2%|▏         | 30/1686 [20:32<20:59:26, 45.63s/it, loss=7.35]\u001b[A\n",
            "  2%|▏         | 30/1686 [21:20<20:59:26, 45.63s/it, loss=7.42]\u001b[A\n",
            "  2%|▏         | 31/1686 [21:20<21:13:34, 46.17s/it, loss=7.42]\u001b[A\n",
            "  2%|▏         | 31/1686 [22:06<21:13:34, 46.17s/it, loss=7.34]\u001b[A\n",
            "  2%|▏         | 32/1686 [22:06<21:16:08, 46.29s/it, loss=7.34]\u001b[A\n",
            "  2%|▏         | 32/1686 [22:53<21:16:08, 46.29s/it, loss=7.25]\u001b[A\n",
            "  2%|▏         | 33/1686 [22:53<21:21:36, 46.52s/it, loss=7.25]\u001b[A\n",
            "  2%|▏         | 33/1686 [23:42<21:21:36, 46.52s/it, loss=7.38]\u001b[A\n",
            "  2%|▏         | 34/1686 [23:42<21:43:06, 47.33s/it, loss=7.38]\u001b[A\n",
            "  2%|▏         | 34/1686 [24:31<21:43:06, 47.33s/it, loss=7.32]\u001b[A\n",
            "  2%|▏         | 35/1686 [24:31<21:56:32, 47.85s/it, loss=7.32]\u001b[A\n",
            "  2%|▏         | 35/1686 [25:20<21:56:32, 47.85s/it, loss=7.26]\u001b[A\n",
            "  2%|▏         | 36/1686 [25:20<22:03:15, 48.12s/it, loss=7.26]\u001b[A\n",
            "  2%|▏         | 36/1686 [26:10<22:03:15, 48.12s/it, loss=7.23]\u001b[A\n",
            "  2%|▏         | 37/1686 [26:10<22:16:25, 48.63s/it, loss=7.23]\u001b[A\n",
            "  2%|▏         | 37/1686 [26:58<22:16:25, 48.63s/it, loss=7.16]\u001b[A\n",
            "  2%|▏         | 38/1686 [26:58<22:11:42, 48.48s/it, loss=7.16]\u001b[A\n",
            "  2%|▏         | 38/1686 [27:48<22:11:42, 48.48s/it, loss=7.23]\u001b[A\n",
            "  2%|▏         | 39/1686 [27:48<22:21:14, 48.86s/it, loss=7.23]\u001b[A\n",
            "  2%|▏         | 39/1686 [28:35<22:21:14, 48.86s/it, loss=7.17]\u001b[A\n",
            "  2%|▏         | 40/1686 [28:35<22:10:21, 48.49s/it, loss=7.17]\u001b[A\n",
            "  2%|▏         | 40/1686 [29:24<22:10:21, 48.49s/it, loss=7.08]\u001b[A\n",
            "  2%|▏         | 41/1686 [29:24<22:10:43, 48.54s/it, loss=7.08]\u001b[A\n",
            "  2%|▏         | 41/1686 [30:13<22:10:43, 48.54s/it, loss=7.05]\u001b[A\n",
            "  2%|▏         | 42/1686 [30:13<22:10:10, 48.55s/it, loss=7.05]\u001b[A\n",
            "  2%|▏         | 42/1686 [31:02<22:10:10, 48.55s/it, loss=7.07]\u001b[A\n",
            "  3%|▎         | 43/1686 [31:02<22:12:38, 48.67s/it, loss=7.07]\u001b[A\n",
            "  3%|▎         | 43/1686 [31:51<22:12:38, 48.67s/it, loss=7.11]\u001b[A\n",
            "  3%|▎         | 44/1686 [31:51<22:17:59, 48.89s/it, loss=7.11]\u001b[A\n",
            "  3%|▎         | 44/1686 [32:41<22:17:59, 48.89s/it, loss=7.07]\u001b[A\n",
            "  3%|▎         | 45/1686 [32:41<22:27:05, 49.25s/it, loss=7.07]\u001b[A\n",
            "  3%|▎         | 45/1686 [33:30<22:27:05, 49.25s/it, loss=7.23]\u001b[A\n",
            "  3%|▎         | 46/1686 [33:30<22:25:27, 49.22s/it, loss=7.23]\u001b[A\n",
            "  3%|▎         | 46/1686 [34:20<22:25:27, 49.22s/it, loss=7.11]\u001b[A\n",
            "  3%|▎         | 47/1686 [34:20<22:27:28, 49.33s/it, loss=7.11]\u001b[A\n",
            "  3%|▎         | 47/1686 [35:10<22:27:28, 49.33s/it, loss=7.08]\u001b[A\n",
            "  3%|▎         | 48/1686 [35:10<22:34:26, 49.61s/it, loss=7.08]\u001b[A\n",
            "  3%|▎         | 48/1686 [36:01<22:34:26, 49.61s/it, loss=7.12]\u001b[A\n",
            "  3%|▎         | 49/1686 [36:01<22:43:43, 49.98s/it, loss=7.12]\u001b[A\n",
            "  3%|▎         | 49/1686 [36:51<22:43:43, 49.98s/it, loss=7.08]\u001b[A\n",
            "  3%|▎         | 50/1686 [36:51<22:46:52, 50.13s/it, loss=7.08]\u001b[A\n",
            "  3%|▎         | 50/1686 [37:41<22:46:52, 50.13s/it, loss=7.04]\u001b[A\n",
            "  3%|▎         | 51/1686 [37:41<22:43:42, 50.04s/it, loss=7.04]\u001b[A\n",
            "  3%|▎         | 51/1686 [38:33<22:43:42, 50.04s/it, loss=7.09]\u001b[A\n",
            "  3%|▎         | 52/1686 [38:33<22:55:16, 50.50s/it, loss=7.09]\u001b[A\n",
            "  3%|▎         | 52/1686 [39:24<22:55:16, 50.50s/it, loss=7.01]\u001b[A\n",
            "  3%|▎         | 53/1686 [39:24<22:57:57, 50.63s/it, loss=7.01]\u001b[A\n",
            "  3%|▎         | 53/1686 [40:15<22:57:57, 50.63s/it, loss=6.98]\u001b[A\n",
            "  3%|▎         | 54/1686 [40:15<22:57:47, 50.65s/it, loss=6.98]\u001b[A\n",
            "  3%|▎         | 54/1686 [41:05<22:57:47, 50.65s/it, loss=7.08]\u001b[A\n",
            "  3%|▎         | 55/1686 [41:05<22:55:15, 50.59s/it, loss=7.08]\u001b[A\n",
            "  3%|▎         | 55/1686 [41:57<22:55:15, 50.59s/it, loss=7.05]\u001b[A\n",
            "  3%|▎         | 56/1686 [41:57<23:04:19, 50.96s/it, loss=7.05]\u001b[A\n",
            "  3%|▎         | 56/1686 [42:51<23:04:19, 50.96s/it, loss=7.12]\u001b[A\n",
            "  3%|▎         | 57/1686 [42:51<23:30:41, 51.96s/it, loss=7.12]\u001b[A\n",
            "  3%|▎         | 57/1686 [43:42<23:30:41, 51.96s/it, loss=6.93]\u001b[A\n",
            "  3%|▎         | 58/1686 [43:42<23:23:07, 51.71s/it, loss=6.93]\u001b[A\n",
            "  3%|▎         | 58/1686 [44:34<23:23:07, 51.71s/it, loss=7]   \u001b[A\n",
            "  3%|▎         | 59/1686 [44:34<23:26:04, 51.85s/it, loss=7]\u001b[A\n",
            "  3%|▎         | 59/1686 [45:27<23:26:04, 51.85s/it, loss=6.96]\u001b[A\n",
            "  4%|▎         | 60/1686 [45:27<23:28:20, 51.97s/it, loss=6.96]\u001b[A\n",
            "  4%|▎         | 60/1686 [46:20<23:28:20, 51.97s/it, loss=6.89]\u001b[A\n",
            "  4%|▎         | 61/1686 [46:20<23:42:04, 52.51s/it, loss=6.89]\u001b[A\n",
            "  4%|▎         | 61/1686 [47:12<23:42:04, 52.51s/it, loss=7.02]\u001b[A\n",
            "  4%|▎         | 62/1686 [47:12<23:32:49, 52.20s/it, loss=7.02]\u001b[A\n",
            "  4%|▎         | 62/1686 [48:07<23:32:49, 52.20s/it, loss=6.99]\u001b[A\n",
            "  4%|▎         | 63/1686 [48:07<23:55:12, 53.06s/it, loss=6.99]\u001b[A\n",
            "  4%|▎         | 63/1686 [49:01<23:55:12, 53.06s/it, loss=6.93]\u001b[A\n",
            "  4%|▍         | 64/1686 [49:01<24:06:19, 53.50s/it, loss=6.93]\u001b[A\n",
            "  4%|▍         | 64/1686 [49:55<24:06:19, 53.50s/it, loss=7]   \u001b[A\n",
            "  4%|▍         | 65/1686 [49:55<24:06:57, 53.56s/it, loss=7]\u001b[A\n",
            "  4%|▍         | 65/1686 [50:50<24:06:57, 53.56s/it, loss=6.97]\u001b[A\n",
            "  4%|▍         | 66/1686 [50:50<24:13:18, 53.83s/it, loss=6.97]\u001b[A\n",
            "  4%|▍         | 66/1686 [51:42<24:13:18, 53.83s/it, loss=6.95]\u001b[A\n",
            "  4%|▍         | 67/1686 [51:42<24:01:45, 53.43s/it, loss=6.95]\u001b[A\n",
            "  4%|▍         | 67/1686 [52:36<24:01:45, 53.43s/it, loss=6.97]\u001b[A\n",
            "  4%|▍         | 68/1686 [52:36<24:04:07, 53.55s/it, loss=6.97]\u001b[A\n",
            "  4%|▍         | 68/1686 [53:30<24:04:07, 53.55s/it, loss=6.97]\u001b[A\n",
            "  4%|▍         | 69/1686 [53:30<24:09:08, 53.77s/it, loss=6.97]\u001b[A\n",
            "  4%|▍         | 69/1686 [54:26<24:09:08, 53.77s/it, loss=6.98]\u001b[A\n",
            "  4%|▍         | 70/1686 [54:26<24:25:17, 54.40s/it, loss=6.98]\u001b[A\n",
            "  4%|▍         | 70/1686 [55:19<24:25:17, 54.40s/it, loss=6.86]\u001b[A\n",
            "  4%|▍         | 71/1686 [55:19<24:08:43, 53.82s/it, loss=6.86]\u001b[A\n",
            "  4%|▍         | 71/1686 [56:09<24:08:43, 53.82s/it, loss=6.83]\u001b[A\n",
            "  4%|▍         | 72/1686 [56:09<23:38:17, 52.72s/it, loss=6.83]\u001b[A\n",
            "  4%|▍         | 72/1686 [57:01<23:38:17, 52.72s/it, loss=6.89]\u001b[A\n",
            "  4%|▍         | 73/1686 [57:01<23:31:14, 52.50s/it, loss=6.89]\u001b[A\n",
            "  4%|▍         | 73/1686 [57:53<23:31:14, 52.50s/it, loss=6.98]\u001b[A\n",
            "  4%|▍         | 74/1686 [57:53<23:28:45, 52.44s/it, loss=6.98]\u001b[A\n",
            "  4%|▍         | 74/1686 [58:45<23:28:45, 52.44s/it, loss=6.88]\u001b[A\n",
            "  4%|▍         | 75/1686 [58:45<23:25:19, 52.34s/it, loss=6.88]\u001b[A\n",
            "  4%|▍         | 75/1686 [59:37<23:25:19, 52.34s/it, loss=6.94]\u001b[A\n",
            "  5%|▍         | 76/1686 [59:37<23:16:47, 52.05s/it, loss=6.94]\u001b[A\n",
            "  5%|▍         | 76/1686 [1:00:26<23:16:47, 52.05s/it, loss=6.84]\u001b[A\n",
            "  5%|▍         | 77/1686 [1:00:26<22:55:21, 51.29s/it, loss=6.84]\u001b[A\n",
            "  5%|▍         | 77/1686 [1:01:16<22:55:21, 51.29s/it, loss=6.82]\u001b[A\n",
            "  5%|▍         | 78/1686 [1:01:16<22:42:48, 50.85s/it, loss=6.82]\u001b[A\n",
            "  5%|▍         | 78/1686 [1:02:07<22:42:48, 50.85s/it, loss=6.89]\u001b[A\n",
            "  5%|▍         | 79/1686 [1:02:07<22:40:27, 50.80s/it, loss=6.89]\u001b[A\n",
            "  5%|▍         | 79/1686 [1:02:56<22:40:27, 50.80s/it, loss=6.88]\u001b[A\n",
            "  5%|▍         | 80/1686 [1:02:56<22:29:05, 50.40s/it, loss=6.88]\u001b[A\n",
            "  5%|▍         | 80/1686 [1:03:44<22:29:05, 50.40s/it, loss=6.91]\u001b[A\n",
            "  5%|▍         | 81/1686 [1:03:44<22:12:27, 49.81s/it, loss=6.91]\u001b[A\n",
            "  5%|▍         | 81/1686 [1:04:33<22:12:27, 49.81s/it, loss=6.8] \u001b[A\n",
            "  5%|▍         | 82/1686 [1:04:33<22:04:37, 49.55s/it, loss=6.8]\u001b[A\n",
            "  5%|▍         | 82/1686 [1:05:23<22:04:37, 49.55s/it, loss=6.86]\u001b[A\n",
            "  5%|▍         | 83/1686 [1:05:23<22:03:46, 49.55s/it, loss=6.86]\u001b[A\n",
            "  5%|▍         | 83/1686 [1:06:13<22:03:46, 49.55s/it, loss=6.92]\u001b[A\n",
            "  5%|▍         | 84/1686 [1:06:13<22:07:14, 49.71s/it, loss=6.92]\u001b[A\n",
            "  5%|▍         | 84/1686 [1:07:02<22:07:14, 49.71s/it, loss=6.91]\u001b[A\n",
            "  5%|▌         | 85/1686 [1:07:02<22:03:44, 49.61s/it, loss=6.91]\u001b[A\n",
            "  5%|▌         | 85/1686 [1:07:54<22:03:44, 49.61s/it, loss=6.88]\u001b[A\n",
            "  5%|▌         | 86/1686 [1:07:54<22:21:30, 50.31s/it, loss=6.88]\u001b[A\n",
            "  5%|▌         | 86/1686 [1:08:44<22:21:30, 50.31s/it, loss=6.85]\u001b[A\n",
            "  5%|▌         | 87/1686 [1:08:44<22:14:21, 50.07s/it, loss=6.85]\u001b[A\n",
            "  5%|▌         | 87/1686 [1:09:33<22:14:21, 50.07s/it, loss=6.85]\u001b[A\n",
            "  5%|▌         | 88/1686 [1:09:33<22:06:06, 49.79s/it, loss=6.85]\u001b[A\n",
            "  5%|▌         | 88/1686 [1:10:21<22:06:06, 49.79s/it, loss=6.84]\u001b[A\n",
            "  5%|▌         | 89/1686 [1:10:21<21:48:01, 49.14s/it, loss=6.84]\u001b[A\n",
            "  5%|▌         | 89/1686 [1:11:10<21:48:01, 49.14s/it, loss=6.8] \u001b[A\n",
            "  5%|▌         | 90/1686 [1:11:10<21:46:36, 49.12s/it, loss=6.8]\u001b[A\n",
            "  5%|▌         | 90/1686 [1:12:00<21:46:36, 49.12s/it, loss=6.83]\u001b[A\n",
            "  5%|▌         | 91/1686 [1:12:00<21:55:44, 49.50s/it, loss=6.83]\u001b[A\n",
            "  5%|▌         | 91/1686 [1:12:49<21:55:44, 49.50s/it, loss=6.82]\u001b[A\n",
            "  5%|▌         | 92/1686 [1:12:49<21:50:04, 49.31s/it, loss=6.82]\u001b[A\n",
            "  5%|▌         | 92/1686 [1:13:39<21:50:04, 49.31s/it, loss=6.83]\u001b[A\n",
            "  6%|▌         | 93/1686 [1:13:39<21:53:44, 49.48s/it, loss=6.83]\u001b[A\n",
            "  6%|▌         | 93/1686 [1:14:28<21:53:44, 49.48s/it, loss=6.86]\u001b[A\n",
            "  6%|▌         | 94/1686 [1:14:28<21:49:53, 49.37s/it, loss=6.86]\u001b[A\n",
            "  6%|▌         | 94/1686 [1:15:19<21:49:53, 49.37s/it, loss=6.93]\u001b[A\n",
            "  6%|▌         | 95/1686 [1:15:19<22:03:52, 49.93s/it, loss=6.93]\u001b[A\n",
            "  6%|▌         | 95/1686 [1:16:12<22:03:52, 49.93s/it, loss=6.82]\u001b[A\n",
            "  6%|▌         | 96/1686 [1:16:12<22:24:22, 50.73s/it, loss=6.82]\u001b[A\n",
            "  6%|▌         | 96/1686 [1:17:03<22:24:22, 50.73s/it, loss=6.83]\u001b[A\n",
            "  6%|▌         | 97/1686 [1:17:03<22:24:07, 50.75s/it, loss=6.83]\u001b[A\n",
            "  6%|▌         | 97/1686 [1:17:51<22:24:07, 50.75s/it, loss=6.83]\u001b[A\n",
            "  6%|▌         | 98/1686 [1:17:51<22:07:41, 50.16s/it, loss=6.83]\u001b[A\n",
            "  6%|▌         | 98/1686 [1:18:42<22:07:41, 50.16s/it, loss=6.84]\u001b[A\n",
            "  6%|▌         | 99/1686 [1:18:42<22:06:53, 50.17s/it, loss=6.84]\u001b[A\n",
            "  6%|▌         | 99/1686 [1:19:30<22:06:53, 50.17s/it, loss=6.73]\u001b[A\n",
            "  6%|▌         | 100/1686 [1:19:30<21:51:48, 49.63s/it, loss=6.73]\u001b[A\n",
            "  6%|▌         | 100/1686 [1:20:18<21:51:48, 49.63s/it, loss=6.81]\u001b[A\n",
            "  6%|▌         | 101/1686 [1:20:18<21:41:30, 49.27s/it, loss=6.81]\u001b[A\n",
            "  6%|▌         | 101/1686 [1:21:06<21:41:30, 49.27s/it, loss=6.77]\u001b[A\n",
            "  6%|▌         | 102/1686 [1:21:06<21:26:35, 48.73s/it, loss=6.77]\u001b[A\n",
            "  6%|▌         | 102/1686 [1:21:53<21:26:35, 48.73s/it, loss=6.86]\u001b[A\n",
            "  6%|▌         | 103/1686 [1:21:53<21:15:36, 48.35s/it, loss=6.86]\u001b[A\n",
            "  6%|▌         | 103/1686 [1:22:41<21:15:36, 48.35s/it, loss=6.74]\u001b[A\n",
            "  6%|▌         | 104/1686 [1:22:41<21:07:00, 48.05s/it, loss=6.74]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-ee618c4699a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\ntrain: {loss}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-af521c073884>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(data_iter, model, criterion)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mcurr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bL0nXYMr9-r",
        "colab_type": "text"
      },
      "source": [
        "Что-то это обучается очень долго:("
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V26LazF4AeTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search(model, src, src_mask, max_len=20, k=5):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    start_token = TGT.vocab.stoi[\"<s>\"], TGT.vocab.stoi[\"</s>\"]\n",
        "    out = torch.ones(1, 1).fill_(start_token).type_as(src.data)\n",
        "    beam = [(out, 0)]\n",
        "    for i in range(max_len):\n",
        "        candidates, candidates_proba, prev_prob = [], [], None\n",
        "        for snt, snt_proba in beam:\n",
        "            if snt[0][-1] == end_token:\n",
        "                candidates.append(snt)\n",
        "                candidates_proba.append(snt_proba)\n",
        "            else:\n",
        "                proba = model.decode(memory, src_mask, snt,\n",
        "                                     subsequent_mask(snt.size(1)).type_as(src.data))\n",
        "                proba = proba[0][i]\n",
        "                best_k = torch.argsort(-proba)[:k].tolist()\n",
        "                proba = proba.tolist()\n",
        "                prev_prob = proba\n",
        "                for tok in best_k:\n",
        "                    candidates.append(torch.cat([snt, torch.ones(1, 1).type_as(src.data).fill_(tok)], dim=1))\n",
        "                    candidates_proba.append(snt_proba + np.log(proba[tok])) \n",
        "         \n",
        "        best_candidates = np.argsort(-np.array(candidates_proba))[:k]\n",
        "        beam = [(candidates[j], candidates_proba[j]) for j in best_candidates]\n",
        "    return beam \n",
        "\n",
        "def top_k(pred):\n",
        "    top_k = 500\n",
        "    top_k = min(top_k, pred.size(-1))\n",
        "    \n",
        "    indices_to_remove = pred < torch.topk(pred, top_k)[0][..., -1, None]\n",
        "    pred[indices_to_remove] = -float('Inf')\n",
        "    probs = torch.softmax(pred, dim=-1)\n",
        "    prev = torch.multinomial(probs, num_samples=1)\n",
        "    return prev\n",
        "\n",
        "#retreived from the code in the article\n",
        "def greedy_decode(model, src, src_mask, max_len=10): #, start_symbol, target):\n",
        "    start_symbol, end_token = TGT.vocab.stoi[\"<s>\"], TGT.vocab.stoi[\"</s>\"]\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        next_word = top_k(prob)[0]\n",
        "        next_word += 2\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "        if TARGET_TXT.vocab.itos[ys[0][-1]] == \"</s>\":\n",
        "          break\n",
        "    return ys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29suOiG1s42J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdK4EVp5WKWq",
        "colab_type": "code",
        "outputId": "b32f3efa-03bf-43dd-947c-30806e7f94b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(valid_iter):\n",
        "        src = batch.src[:1]\n",
        "        src_key_padding_mask = src != SRC.vocab.stoi[\"<pad>\"]\n",
        "        beam = beam_search(model, src, src_key_padding_mask)\n",
        "        \n",
        "        seq = []\n",
        "        for i in range(1, src.size(1)):\n",
        "            sym = SRC.vocab.itos[src[0, i]]\n",
        "            if sym == \"</s>\": break\n",
        "            seq.append(sym)\n",
        "        seq = tok_ru.decode_pieces(seq)\n",
        "        print(\"\\nSource:\", seq)\n",
        "        \n",
        "        print(\"Translation:\")\n",
        "        for pred, pred_proba in beam:                \n",
        "            seq = []\n",
        "            for i in range(1, pred.size(1)):\n",
        "                sym = TGT.vocab.itos[pred[0, i]]\n",
        "                if sym == \"</s>\": break\n",
        "                seq.append(sym)\n",
        "            seq = tok_en.decode_pieces(seq)\n",
        "            print(f\"pred {pred_proba:.2f}:\", seq)\n",
        "                \n",
        "        seq = []\n",
        "        for i in range(1, batch.trg.size(1)):\n",
        "            sym = TGT.vocab.itos[batch.trg[0, i]]\n",
        "            if sym == \"</s>\": break\n",
        "            seq.append(sym)\n",
        "        seq = tok_en.decode_pieces(seq)\n",
        "        print(\"Target:\", seq)\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-462669ac467e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msrc_key_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mSRC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<pad>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-111-d422bd0ef4ca>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         return map(\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mlambda\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTGT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-110-8ba4fd70192d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, trg, pad)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_std_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_y\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-8ba4fd70192d>\u001b[0m in \u001b[0;36mmake_std_mask\u001b[0;34m(tgt, pad)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"Create a mask to hide padding and future words.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         tgt_mask = tgt_mask & Variable(\n\u001b[0m\u001b[1;32m     17\u001b[0m             subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Variable' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzIV7MrjtMMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk import translate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViDe_L08tMv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_iter):\n",
        "        for j, el in enumerate(batch.src):\n",
        "            src = batch.src[j:j+1]\n",
        "            src_key_padding_mask = src != SRC.vocab.stoi[\"<pad>\"]\n",
        "            beam = beam_search(model, src, src_key_padding_mask)\n",
        "            for pred, pred_proba in beam[:1]:                \n",
        "                seq = []\n",
        "                for i in range(1, pred.size(1)):\n",
        "                    sym = TGT.vocab.itos[pred[0, i]]\n",
        "                    if sym == \"</s>\": break\n",
        "                    seq.append(sym)\n",
        "                seq = tok_en.decode_pieces(seq)\n",
        "                trg = batch.trg[j:j+1].tolist()[0]\n",
        "                ref = []\n",
        "                for i, se in enumerate(trg):\n",
        "                     tok = TGT.vocab.itos[trg[i]]\n",
        "                     if tok == \"<pad>\": break\n",
        "                     ref.append(tok)\n",
        "                hypotheses.append(seq.split())\n",
        "                references.append(tok_en.decode_pieces(ref).split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMY1HTHstbWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_bleu(references, hypotheses, \n",
        "            smoothing_function=translate.bleu_score.SmoothingFunction().method3,\n",
        "            auto_reweigh=True\n",
        "           )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}