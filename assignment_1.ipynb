{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWrA1rwRTdRy"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "Using text http://www.gutenberg.org/files/2600/2600-0.txt\n",
    "1. Make text lowercase and remove all punctuation except spaces and dots.\n",
    "2. Tokenize text by BPE with vocab_size = 100\n",
    "3. Train 3-gram language model with laplace smoothing $\\delta=1$\n",
    "4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens.\n",
    "5. Calculate perplexity of the language model for the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_hSV_B6h2cv"
   },
   "outputs": [],
   "source": [
    "text = open('/Users/victoriaregina/Documents/assignment1/text.txt', 'r').read()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQq9GityTdSB"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "  # TODO\n",
    "  # make lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "  # replace all punctuation except dots with spaces\n",
    "    text = \"\".join(l for l in text if l not in '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~»«—…‘’“”')\n",
    "  \n",
    "  # collapse multiple spaces into one '   ' -> ' '\n",
    "    text = ' '.join(text.split())\n",
    " \n",
    "    return text\n",
    "\n",
    "\n",
    "text2 = preprocess_text(text)\n",
    "itos = list(set(text2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0kDnGoqTdSJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30876"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3 = text2.split('.')\n",
    "text3 = [x.strip() for x in text3]\n",
    "len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-M5zNO1pTdSQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class BPE(TransformerMixin):\n",
    "    def __init__(self, vocab_size=100):\n",
    "        super(BPE, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # index to token\n",
    "        self.itos = []\n",
    "        # token to index\n",
    "        self.stoi = {}\n",
    "        \n",
    "    def fit(self, text):\n",
    "        syms = set()\n",
    "      \n",
    "      # tokenize text by symbols and fill in self.itos and self.stoi\n",
    "        for phrase in text:\n",
    "            for symb in phrase:\n",
    "                syms.add(symb)\n",
    "          \n",
    "        self.itos = list(syms)\n",
    "\n",
    "        for n_sym, sym in enumerate(self.itos):\n",
    "            self.stoi[sym] = n_sym\n",
    "      \n",
    "        coded_text = []\n",
    "        for string in text:\n",
    "            \n",
    "            coded_string = []\n",
    "            for s in string:\n",
    "                coded_string.append(self.stoi[s])\n",
    "            \n",
    "            coded_text.append(coded_string)\n",
    "   \n",
    "        text4 = coded_text\n",
    "        \n",
    "        while len(self.itos) < self.vocab_size:\n",
    "            #print(len(self.itos))\n",
    "            pairs = {}\n",
    "\n",
    "      # count bigram freqencies in the text\n",
    "\n",
    "            for sentence in text4:\n",
    "              for i, t in enumerate(sentence):\n",
    "                if i < len(sentence)-1:\n",
    "                    p = (sentence[i], sentence[i+1])\n",
    "                    if p not in pairs:\n",
    "                        pairs[p] = 1\n",
    "                    else:\n",
    "                        pairs[p] += 1\n",
    "          \n",
    "          # most common bigram in the text\n",
    "        \n",
    "            most_freq_bigram = sorted(pairs.items(), key=lambda x: (x[1],x[0]), reverse=True)[0][0]\n",
    "\n",
    "            \n",
    "          #convert most frequent bigram into new_token to find it in the text\n",
    "            new_token = ''\n",
    "            for e in most_freq_bigram:\n",
    "                #print(e)\n",
    "                new_token = new_token + ' ' + str(e)\n",
    "                #print(new_token)\n",
    "            \n",
    "            self.itos.append(new_token)\n",
    "            self.stoi[new_token] = len(self.itos)\n",
    "\n",
    "          #find occurences of the new_token in the text and replace them with new_id\n",
    "            text5 = []\n",
    "            for string in text4:\n",
    "                stroka_arr = []\n",
    "                for s in string:\n",
    "                    stroka_arr.append(str(s))\n",
    "                    stroka = ' '.join(stroka_arr)\n",
    "                #print(stroka)\n",
    "                \n",
    "            #print('changed string', stroka)\n",
    "            \n",
    "                #stroka = stroka.replace(new_token, ' ' + str(self.stoi[most_freq_bigram]))\n",
    "                stroka = stroka.replace(new_token, ' ' + str(self.stoi[new_token]))\n",
    "                temp_st = []\n",
    "                for s in stroka.split():\n",
    "                    temp_st.append(int(s))\n",
    "                text5.append(temp_st)\n",
    "            #print('temp', text5)\n",
    "\n",
    "            text4 = text5\n",
    "          \n",
    "        return self\n",
    "    \n",
    "    def transform(self, text):\n",
    "        \n",
    "        #tokenize text by symbols using self.stoi\n",
    "        text_transformed = []\n",
    "    \n",
    "        for string in text:\n",
    "       \n",
    "        #начальный элемент массива Рез\n",
    "            arRes = []\n",
    "            arStr = list(string)\n",
    "            if len(arStr) > 1:\n",
    "                #print(len(arStr))\n",
    "        \n",
    "                if (self.stoi[arStr[0]], self.stoi[arStr[1]]) in self.stoi:\n",
    "                    p = (self.stoi[arStr[0]], self.stoi[arStr[1]])\n",
    "                    arRes.append(p)\n",
    "\n",
    "                else:\n",
    "                    arRes.append(arStr[0])\n",
    "\n",
    "                arStr = list(string)\n",
    "            # сравниваем пару: последний элемент рез-а и текущий элемент строки\n",
    "            # если есть такая пара вместо последнего Рез-а пишем значение пары, иначе \n",
    "            # добавляем новый элемент в Рез\n",
    "                i = 1                   \n",
    "                while i <= len(arStr)-1:\n",
    "                    lastIndex = len(arRes)-1\n",
    "                    lastEl = arRes[lastIndex] \n",
    "                    s = arStr[i]\n",
    "\n",
    "                    if (self.stoi[lastEl], self.stoi[s]) in self.stoi:\n",
    "                        p = (self.stoi[lastEl], self.stoi[s])\n",
    "                        arRes[lastIndex] = p\n",
    "\n",
    "                    else:\n",
    "                        arRes.append(s)\n",
    "                    i += 1\n",
    "\n",
    "            #перевод резов в цифру\n",
    "\n",
    "          #find occurences of the token in the text and replace them with token_id\n",
    "\n",
    "                for num, token in enumerate(self.itos):\n",
    "                    if token in arRes:\n",
    "                        for i, el in enumerate(arRes):\n",
    "                            if el == token:\n",
    "                                arRes[i] = num\n",
    "                text_transformed.append(arRes)\n",
    "\n",
    "        return text_transformed\n",
    "    \n",
    "    \n",
    "    def decode_token(self, tok): \n",
    "        \"\"\"\n",
    "        tok: int or tuple\n",
    "        \"\"\"\n",
    "        result = self.itos[tok] # TODO\n",
    "        return result \n",
    "            \n",
    "    def decode(self, text):\n",
    "        \"\"\"\n",
    "        convert token ids into text\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        return ''.join(map(self.decode_token, text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "aSWFz27dlJwv",
    "outputId": "39813303-7fec-478e-e112-6004aa925f3a"
   },
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "bpe = BPE(vocab_size)\n",
    "tokenized_text = bpe.fit_transform(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bpe.decode(tokenized_text[0]) == text3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "omLs6viyTdSY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "  \n",
    "start_token = vocab_size\n",
    "end_token = vocab_size + 1\n",
    "        \n",
    "    \n",
    "class LM:\n",
    "    def __init__(self, vocab_size, delta=1):\n",
    "        self.delta = delta\n",
    "        self.vocab_size = vocab_size + 2\n",
    "        # create array for storing 3-gram counters\n",
    "        self.proba = Counter()\n",
    "        \n",
    "        \n",
    "    def infer(self, a, b, tau=1):\n",
    "        \"\"\"\n",
    "        return vector of probabilities of size self.vocab for 3-grams which start with (a,b) tokens\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        \n",
    "        freqs = []\n",
    "        freq_sum = 0\n",
    "        for word in range(self.vocab_size):\n",
    "            word_counter = self.get_proba(a, b, word, tau=tau)\n",
    "            freqs.append(word_counter)\n",
    "            freq_sum += word_counter\n",
    "        result = [f/freq_sum for f in freqs]\n",
    "        \n",
    "        return np.array(result)\n",
    "        \n",
    "    def get_proba(self, a, b, c, tau=1):\n",
    "        \"\"\"\n",
    "        get probability of 3-gram (a,b,c)\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        c: third token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        # approximate probability by counters\n",
    "        \n",
    "        trigram =  self.proba[(a, b, c)]\n",
    "        result = (trigram + self.delta) ** (1/tau) \n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        train language model on text\n",
    "        text: list of lists\n",
    "        \"\"\"\n",
    "        # count 3-grams in the text\n",
    "        \n",
    "        for t in text:\n",
    "            #for e in t:\n",
    "            t = [start_token] + [t_el for t_el in t] + [end_token]\n",
    "                #t = [start_token] + [e] + [end_token]\n",
    "                \n",
    "            self.proba.update((zip(t, t[1:], t[2:])))\n",
    "            \n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LM(vocab_size, 1).fit(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
    "    \"\"\"\n",
    "    generate sequence from language model *lm* conditioned on input_seq\n",
    "    input_seq: sequence of token ids for conditioning\n",
    "    lm: language model\n",
    "    max_len: max generated sequence length\n",
    "    k: size of beam\n",
    "    tau: temperature\n",
    "    \"\"\"\n",
    "    beam = [(input_seq, 0)] # TODO store in beam tuples of current sequences and their log probabilities\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        candidates = []\n",
    "        candidates_proba = []\n",
    "        for snt, snt_proba in beam:\n",
    "            if np.argmax(snt[-1]) == end_token:\n",
    "                continue\n",
    "            else:\n",
    "                proba = lm.infer(snt[-2], snt[-1], tau) # probability vector of the next token\n",
    "                \n",
    "                best_k = proba.argsort()[-k:][::-1] # top-k most probable tokens\n",
    "                \n",
    "                # TODO update candidates' sequences and corresponding probabilities\n",
    "                    \n",
    "                candidates += [snt + [int(best)] for best in best_k]\n",
    "                candidates_proba += [snt_proba + best for best in np.log(proba[best_k])]\n",
    "\n",
    "        # select top-k most probable sequences from candidates\n",
    "        beam = [(candidates[i], candidates_proba[i]) for i in np.argsort(candidates_proba).astype(int)[-k:][::-1]]\n",
    "                \n",
    "    return beam\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6GqnwRHWTdSk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of the sequence: horse and the an is 0.44203236747819247\n",
      "probability of the sequence: horse the and th is 0.13678898877003134\n",
      "probability of the sequence: horse and the th is 0.13678898877003132\n",
      "probability of the sequence: horse the the an is 0.028786621492962825\n",
      "probability of the sequence: horse and the sa is 0.018796482870682373\n",
      "probability of the sequence: horse and the so is 0.015418175164731457\n",
      "probability of the sequence: horse said the a is 0.014567315524745517\n",
      "probability of the sequence: horse and the wh is 0.012442495231459908\n",
      "probability of the sequence: horse was and th is 0.010980752280501927\n",
      "probability of the sequence: horse and the sh is 0.00921311886762556\n"
     ]
    }
   ],
   "source": [
    "input1 = 'horse '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "for (snt, snt_proba) in result:\n",
    "    print('probability of the sequence:', bpe.decode(snt), 'is', np.exp(snt_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "em_RdTnHTdSn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of the sequence: her the and t is 0.46393689605804317\n",
      "probability of the sequence: her and the a is 0.19057696585337455\n",
      "probability of the sequence: her the the a is 0.09763576857881755\n",
      "probability of the sequence: her and the t is 0.05897351279437877\n",
      "probability of the sequence: her the the t is 0.030213117423130327\n",
      "probability of the sequence: her and the s is 0.020221051706612227\n",
      "probability of the sequence: her the said  is 0.015520067698379773\n",
      "probability of the sequence: her the the s is 0.010359583153223632\n",
      "probability of the sequence: her and the w is 0.009842230386299578\n",
      "probability of the sequence: her the so th is 0.00937638466403029\n"
     ]
    }
   ],
   "source": [
    "input1 = 'her'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "for (snt, snt_proba) in result:\n",
    "    print('probability of the sequence:', bpe.decode(snt), 'is', np.exp(snt_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBjMvsq4TdSq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of the sequence: what of the th is 0.00010492944469129061\n",
      "probability of the sequence: what the and t is 9.515907418415685e-05\n",
      "probability of the sequence: what the the a is 7.299435842341057e-05\n",
      "probability of the sequence: what of the an is 7.269115019068714e-05\n",
      "probability of the sequence: what the the t is 6.491543245869236e-05\n",
      "probability of the sequence: what the ther  is 6.356741232950716e-05\n",
      "probability of the sequence: what the and a is 6.179151476380372e-05\n",
      "probability of the sequence: what and the a is 6.068306031511275e-05\n",
      "probability of the sequence: what the the s is 5.8326058468719196e-05\n",
      "probability of the sequence: what the the w is 5.42739721645402e-05\n"
     ]
    }
   ],
   "source": [
    "input1 = 'what'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=1)\n",
    "for (snt, snt_proba) in result:\n",
    "    print('probability of the sequence:', bpe.decode(snt), 'is', np.exp(snt_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tcr6JLamTdSu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of the sequence: gun the and th is 0.6438173769600412\n",
      "probability of the sequence: gun the the an is 0.13548844324230633\n",
      "probability of the sequence: gun the the th is 0.041927534055648544\n",
      "probability of the sequence: gun the said t is 0.021216691990735045\n",
      "probability of the sequence: gun and the an is 0.01580860422180541\n",
      "probability of the sequence: gun the so the is 0.013012211812235578\n",
      "probability of the sequence: gun the was an is 0.010876350834173001\n",
      "probability of the sequence: gun the she an is 0.009120817512504905\n",
      "probability of the sequence: gun the and an is 0.008581012845634669\n",
      "probability of the sequence: gun the the sa is 0.005761356837076132\n"
     ]
    }
   ],
   "source": [
    "input1 = 'gun '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "for (snt, snt_proba) in result:\n",
    "    print('probability of the sequence:', bpe.decode(snt), 'is', np.exp(snt_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lf9DTucoTdS0"
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from math import log\n",
    "\n",
    "def perplexity(snt, lm):\n",
    "    \n",
    "    \"\"\"\n",
    "    snt: sequence of token ids\n",
    "    lm: language model\n",
    "    \"\"\"\n",
    "    \n",
    "    sigma = 0\n",
    "    \n",
    "    snt = [start_token] + snt + [end_token]\n",
    "    #print(snt)\n",
    "\n",
    "    for sym in range(len(snt) - 2):\n",
    "        sigma += log((1 / lm.infer(snt[sym], snt[sym + 1])[snt[sym + 2]]))\n",
    "    factor = (-1 / float(len(snt))) * sigma\n",
    "    result = 2 ** factor\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23800720528860603"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(tokenized_text[0], lm)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
