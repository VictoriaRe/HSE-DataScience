{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 9.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOqmohogjuO32Hotq1Yie31",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VictoriaRe/HSE-DataScience/blob/master/Assignment9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQs5WhOZ3ux-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "3a5d16db-5805-46c6-ba2e-bb7d9f7dc371"
      },
      "source": [
        "!wget -O data.zip https://github.com/thedenaas/hse_seminars/blob/master/2018/seminar_13/data.zip?raw=true\n",
        "!unzip data.zip"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-22 17:29:17--  https://github.com/thedenaas/hse_seminars/blob/master/2018/seminar_13/data.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/thedenaas/hse_seminars/raw/master/2018/seminar_13/data.zip [following]\n",
            "--2020-03-22 17:29:18--  https://github.com/thedenaas/hse_seminars/raw/master/2018/seminar_13/data.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/thedenaas/hse_seminars/master/2018/seminar_13/data.zip [following]\n",
            "--2020-03-22 17:29:18--  https://raw.githubusercontent.com/thedenaas/hse_seminars/master/2018/seminar_13/data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9927168 (9.5M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   9.47M  25.7MB/s    in 0.4s    \n",
            "\n",
            "2020-03-22 17:29:19 (25.7 MB/s) - ‘data.zip’ saved [9927168/9927168]\n",
            "\n",
            "Archive:  data.zip\n",
            "replace data.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace stopwords.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8Ss0Dz5HUQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data.txt', 'r', encoding='utf-8') as txt_file:\n",
        "    data = txt_file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNQ5EnRsHgys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8c4d15a5-8c8f-48ae-a3ea-f72787cd7820"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6QqB5byIgxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(data):\n",
        "  return nltk.word_tokenize(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyE6r6IIHbuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"stopwords.txt\", \"r\" ) as f:\n",
        "    stopwords=[line.strip().lower() for line in f.readlines()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX2Echi2awVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkHT2mWpfXgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pandas.DataFrame()\n",
        "train_data['positive'] = nltk.sent_tokenize(data)[:800] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPA5YDfOfy7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data['negative_1'] = train_data['positive'].apply(lambda n: train_data.iloc[np.random.choice(800), 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfNmQ4Z-hAdh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "454a54df-d28c-429d-e506-e960e50ab14e"
      },
      "source": [
        "train_data"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>positive</th>\n",
              "      <th>negative_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Barclays' defiance of US fines has merit Barcl...</td>\n",
              "      <td>The Dallas police chief, David Brown, recently...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So it is tempting to think the bank, when aske...</td>\n",
              "      <td>It was one of the first email services not tie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>That is not the view of the chief executive, J...</td>\n",
              "      <td>The suspension of Spencer, along with the acco...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Barclays thinks the DoJ’s claims are “disconne...</td>\n",
              "      <td>In December, Defense Secretary Ashton Carter a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>But actually, some grudging respect for Staley...</td>\n",
              "      <td>Low interest rates, accompanied by piles of go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>Frances Perraudin had some more details earlier.</td>\n",
              "      <td>“I think Bernie ought to be president – of Ben...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>Remain campaigners in Islington, a Labour stro...</td>\n",
              "      <td>“Had the campaign known of her full filmograph...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>The ’s David Pegg, who is anchored in deepest ...</td>\n",
              "      <td>Barclays thinks the DoJ’s claims are “disconne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>“Turnout appears to have been fairly high.</td>\n",
              "      <td>The Italian referendum proved a non-event for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>An awful lot of people are saying ‘I’ve alread...</td>\n",
              "      <td>Emails become a standard - 1973 The first emai...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              positive                                         negative_1\n",
              "0    Barclays' defiance of US fines has merit Barcl...  The Dallas police chief, David Brown, recently...\n",
              "1    So it is tempting to think the bank, when aske...  It was one of the first email services not tie...\n",
              "2    That is not the view of the chief executive, J...  The suspension of Spencer, along with the acco...\n",
              "3    Barclays thinks the DoJ’s claims are “disconne...  In December, Defense Secretary Ashton Carter a...\n",
              "4    But actually, some grudging respect for Staley...  Low interest rates, accompanied by piles of go...\n",
              "..                                                 ...                                                ...\n",
              "795   Frances Perraudin had some more details earlier.  “I think Bernie ought to be president – of Ben...\n",
              "796  Remain campaigners in Islington, a Labour stro...  “Had the campaign known of her full filmograph...\n",
              "797  The ’s David Pegg, who is anchored in deepest ...  Barclays thinks the DoJ’s claims are “disconne...\n",
              "798         “Turnout appears to have been fairly high.  The Italian referendum proved a non-event for ...\n",
              "799  An awful lot of people are saying ‘I’ve alread...  Emails become a standard - 1973 The first emai...\n",
              "\n",
              "[800 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGjrECBXhFGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data['negative_2'] = train_data['positive'].apply(lambda n: train_data.iloc[np.random.choice(800), 0])\n",
        "train_data['negative_3'] = train_data['positive'].apply(lambda n: train_data.iloc[np.random.choice(800), 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QvNkGzkhI7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "c56bd2a8-373d-421a-a7cb-6bb203ffb7a3"
      },
      "source": [
        "train_data"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>positive</th>\n",
              "      <th>negative_1</th>\n",
              "      <th>negative_2</th>\n",
              "      <th>negative_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Barclays' defiance of US fines has merit Barcl...</td>\n",
              "      <td>The Dallas police chief, David Brown, recently...</td>\n",
              "      <td>He was born in Florida, but spent some of his ...</td>\n",
              "      <td>Lotus Notes launched - 1989 The first version ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So it is tempting to think the bank, when aske...</td>\n",
              "      <td>It was one of the first email services not tie...</td>\n",
              "      <td>The DoJ lawsuit says Barclays employees called...</td>\n",
              "      <td>The soundtrack to that ad is considered one of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>That is not the view of the chief executive, J...</td>\n",
              "      <td>The suspension of Spencer, along with the acco...</td>\n",
              "      <td>Until that point pre-capacitive consumer smart...</td>\n",
              "      <td>When we temporarily suspend multiple accounts ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Barclays thinks the DoJ’s claims are “disconne...</td>\n",
              "      <td>In December, Defense Secretary Ashton Carter a...</td>\n",
              "      <td>They’re writing off March 8.</td>\n",
              "      <td>“All this new knowledge is useful in the sense...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>But actually, some grudging respect for Staley...</td>\n",
              "      <td>Low interest rates, accompanied by piles of go...</td>\n",
              "      <td>It is understood that most clubs largely agree...</td>\n",
              "      <td>Most stock indices are up in early trading, le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>Frances Perraudin had some more details earlier.</td>\n",
              "      <td>“I think Bernie ought to be president – of Ben...</td>\n",
              "      <td>Yes, even after two years of exposure to the a...</td>\n",
              "      <td>It is immoral.” In the aftermath, Senator Mike...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>Remain campaigners in Islington, a Labour stro...</td>\n",
              "      <td>“Had the campaign known of her full filmograph...</td>\n",
              "      <td>European Union referendum polling day – as it ...</td>\n",
              "      <td>On a weekend that saw yet another mass shootin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>The ’s David Pegg, who is anchored in deepest ...</td>\n",
              "      <td>Barclays thinks the DoJ’s claims are “disconne...</td>\n",
              "      <td>Fitch said: The Negative Outlook for the Itali...</td>\n",
              "      <td>“Serve your community, don’t be a part of the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>“Turnout appears to have been fairly high.</td>\n",
              "      <td>The Italian referendum proved a non-event for ...</td>\n",
              "      <td>It is understood that while no vote was taken ...</td>\n",
              "      <td>In the league, City have not had consecutive v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>An awful lot of people are saying ‘I’ve alread...</td>\n",
              "      <td>Emails become a standard - 1973 The first emai...</td>\n",
              "      <td>US election 2016: Republican party nominates D...</td>\n",
              "      <td>So you can see why we need to ask for your help.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              positive  ...                                         negative_3\n",
              "0    Barclays' defiance of US fines has merit Barcl...  ...  Lotus Notes launched - 1989 The first version ...\n",
              "1    So it is tempting to think the bank, when aske...  ...  The soundtrack to that ad is considered one of...\n",
              "2    That is not the view of the chief executive, J...  ...  When we temporarily suspend multiple accounts ...\n",
              "3    Barclays thinks the DoJ’s claims are “disconne...  ...  “All this new knowledge is useful in the sense...\n",
              "4    But actually, some grudging respect for Staley...  ...  Most stock indices are up in early trading, le...\n",
              "..                                                 ...  ...                                                ...\n",
              "795   Frances Perraudin had some more details earlier.  ...  It is immoral.” In the aftermath, Senator Mike...\n",
              "796  Remain campaigners in Islington, a Labour stro...  ...  On a weekend that saw yet another mass shootin...\n",
              "797  The ’s David Pegg, who is anchored in deepest ...  ...  “Serve your community, don’t be a part of the ...\n",
              "798         “Turnout appears to have been fairly high.  ...  In the league, City have not had consecutive v...\n",
              "799  An awful lot of people are saying ‘I’ve alread...  ...   So you can see why we need to ask for your help.\n",
              "\n",
              "[800 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiaw8uiEIOXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import Field, LabelField, BucketIterator, TabularDataset, Iterator\n",
        "\n",
        "TEXT = Field(include_lengths=False, \n",
        "             batch_first=True, \n",
        "             tokenize=tokenize,\n",
        "             lower=True,\n",
        "             stop_words=stopwords)\n",
        "\n",
        "datafields = [('text',TEXT)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFIMtBycIyzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.to_csv('dataset.csv', index=False)\n",
        "train = TabularDataset(path=\"dataset.csv\", format='csv',\n",
        "                     skip_header=True, fields=datafields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E2yzcFAKO9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwsr8gVwKViR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = Iterator(train, 256, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLmxEzm5ORoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQBY0FapyDyF",
        "colab_type": "text"
      },
      "source": [
        "Имплементация взята отсюда: https://github.com/alexeyev/abae-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUluV0TQOiIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import init\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "class SelfAttention(torch.nn.Module):\n",
        "    def __init__(self, wv_dim, maxlen):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.wv_dim = wv_dim\n",
        "\n",
        "        # max sentence length -- batch 2nd dim size\n",
        "        self.maxlen = maxlen\n",
        "        self.M = Parameter(torch.Tensor(wv_dim, wv_dim))\n",
        "        init.kaiming_uniform(self.M.data)\n",
        "\n",
        "        # softmax for attending to wod vectors\n",
        "        self.attention_softmax = torch.nn.Softmax()\n",
        "\n",
        "    def forward(self, input_embeddings):\n",
        "        # (b, wv, 1)\n",
        "        mean_embedding = torch.mean(input_embeddings, (1,)).unsqueeze(2)\n",
        "\n",
        "        # (wv, wv) x (b, wv, 1) -> (b, wv, 1)\n",
        "        product_1 = torch.matmul(self.M, mean_embedding)\n",
        "\n",
        "        # (b, maxlen, wv) x (b, wv, 1) -> (b, maxlen, 1)\n",
        "        product_2 = torch.matmul(input_embeddings, product_1).squeeze(2)\n",
        "\n",
        "        results = self.attention_softmax(product_2)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return 'wv_dim={}, maxlen={}'.format(self.wv_dim, self.maxlen)\n",
        "\n",
        "\n",
        "class ABAE(torch.nn.Module):\n",
        "    \"\"\"\n",
        "        The model described in the paper ``An Unsupervised Neural Attention Model for Aspect Extraction''\n",
        "        by He, Ruidan and  Lee, Wee Sun  and  Ng, Hwee Tou  and  Dahlmeier, Daniel, ACL2017\n",
        "        https://aclweb.org/anthology/papers/P/P17/P17-1036/\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wv_dim=200, asp_count=30, ortho_reg=0.1, maxlen=201, init_aspects_matrix=None):\n",
        "        \"\"\"\n",
        "        Initializing the model\n",
        "\n",
        "        :param wv_dim: word vector size\n",
        "        :param asp_count: number of aspects\n",
        "        :param ortho_reg: coefficient for tuning the ortho-regularizer's influence\n",
        "        :param maxlen: sentence max length taken into account\n",
        "        :param init_aspects_matrix: None or init. matrix for aspects\n",
        "        \"\"\"\n",
        "        super(ABAE, self).__init__()\n",
        "        self.wv_dim = wv_dim\n",
        "        self.asp_count = asp_count\n",
        "        self.ortho = ortho_reg\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "        self.attention = SelfAttention(wv_dim, maxlen)\n",
        "        self.linear_transform = torch.nn.Linear(self.wv_dim, self.asp_count)\n",
        "        self.softmax_aspects = torch.nn.Softmax()\n",
        "        self.aspects_embeddings = Parameter(torch.Tensor(wv_dim, asp_count))\n",
        "\n",
        "        if init_aspects_matrix is None:\n",
        "            torch.nn.init.xavier_uniform(self.aspects_embeddings)\n",
        "        else:\n",
        "            self.aspects_embeddings.data = torch.from_numpy(init_aspects_matrix.T)\n",
        "\n",
        "    def get_aspects_importances(self, text_embeddings):\n",
        "        \"\"\"\n",
        "            Takes embeddings of a sentence as input, returns attention weights\n",
        "        \"\"\"\n",
        "\n",
        "        # compute attention scores, looking at text embeddings average\n",
        "        attention_weights = self.attention(text_embeddings)\n",
        "\n",
        "        # multiplying text embeddings by attention scores -- and summing\n",
        "        # (matmul: we sum every word embedding's coordinate with attention weights)\n",
        "        weighted_text_emb = torch.matmul(attention_weights.unsqueeze(1), # (batch, 1, sentence)\n",
        "                                         text_embeddings                 # (batch, sentence, wv_dim)\n",
        "                                         ).squeeze()\n",
        "\n",
        "        # encoding with a simple feed-forward layer (wv_dim) -> (aspects_count)\n",
        "        raw_importances = self.linear_transform(weighted_text_emb)\n",
        "\n",
        "        # computing 'aspects distribution in a sentence'\n",
        "        aspects_importances = self.softmax_aspects(raw_importances)\n",
        "\n",
        "        return attention_weights, aspects_importances, weighted_text_emb\n",
        "\n",
        "    def forward(self, text_embeddings, negative_samples_texts):\n",
        "\n",
        "        # negative samples are averaged\n",
        "        averaged_negative_samples = torch.mean(negative_samples_texts, dim=2)\n",
        "\n",
        "        # encoding: words embeddings -> sentence embedding, aspects importances\n",
        "        _, aspects_importances, weighted_text_emb = self.get_aspects_importances(text_embeddings)\n",
        "\n",
        "        # decoding: aspects embeddings matrix, aspects_importances -> recovered sentence embedding\n",
        "        recovered_emb = torch.matmul(self.aspects_embeddings, aspects_importances.unsqueeze(2)).squeeze()\n",
        "\n",
        "        # loss\n",
        "        reconstruction_triplet_loss = ABAE._reconstruction_loss(weighted_text_emb,\n",
        "                                                                recovered_emb,\n",
        "                                                                averaged_negative_samples)\n",
        "        max_margin = torch.max(reconstruction_triplet_loss, torch.zeros_like(reconstruction_triplet_loss))\n",
        "\n",
        "        return self.ortho * self._ortho_regularizer() + max_margin\n",
        "\n",
        "    @staticmethod\n",
        "    def _reconstruction_loss(text_emb, recovered_emb, averaged_negative_emb):\n",
        "\n",
        "        positive_dot_products = torch.matmul(text_emb.unsqueeze(1), recovered_emb.unsqueeze(2)).squeeze()\n",
        "        negative_dot_products = torch.matmul(averaged_negative_emb, recovered_emb.unsqueeze(2)).squeeze()\n",
        "        reconstruction_triplet_loss = torch.sum(1 - positive_dot_products.unsqueeze(1) + negative_dot_products, dim=1)\n",
        "\n",
        "        return reconstruction_triplet_loss\n",
        "\n",
        "    def _ortho_regularizer(self):\n",
        "        return torch.norm(\n",
        "            torch.matmul(self.aspects_embeddings.t(), self.aspects_embeddings) \\\n",
        "            - torch.eye(self.asp_count))\n",
        "\n",
        "    def get_aspect_words(self, w2v_model, topn=15):\n",
        "        words = []\n",
        "\n",
        "        # getting aspects embeddings\n",
        "        aspects = self.aspects_embeddings.detach().numpy()\n",
        "\n",
        "        # getting scalar products of word embeddings and aspect embeddings;\n",
        "        # to obtain the ``probabilities'', one should also apply softmax\n",
        "        words_scores = w2v_model.wv.syn0.dot(aspects)\n",
        "\n",
        "        for row in range(aspects.shape[1]):\n",
        "            argmax_scalar_products = np.argsort(- words_scores[:, row])[:topn]\n",
        "            # print([w2v_model.wv.index2word[i] for i in argmax_scalar_products])\n",
        "            # print([w for w, dist in w2v_model.similar_by_vector(aspects.T[row])[:topn]])\n",
        "            words.append([w2v_model.wv.index2word[i] for i in argmax_scalar_products])\n",
        "\n",
        "        return words\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}